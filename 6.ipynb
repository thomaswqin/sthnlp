{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Let's Do Something with NLP #6 - doc2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://web.stanford.edu/class/cs224n/images/treeFrontSentiment.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[CS224n: Natural Language Processing with Deep Learning](http://web.stanford.edu/class/cs224n/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Jupyter ♥ Reveal.js = RISE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [damianavila/RISE: RISE: \"Live\" Reveal.js Jupyter/IPython Slideshow Extension](https://github.com/damianavila/RISE)\n",
    "- [IPython Revealjs Attacks Again, But Now It Is Alive | SciPy 2014 | Damián Avila - YouTube](https://www.youtube.com/watch?v=sZBKruEh0jI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://camo.githubusercontent.com/0752ef9e293123a9eb986b696030954dcf74e3ef/68747470733a2f2f6d656469612e67697068792e636f6d2f6d656469612f336f78485174547841615a774d4f487239752f67697068792e676966)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## This Week's Papers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Le & Mikolov \"[Distributed Representations of Sentences and Documents](https://cs.stanford.edu/~quocle/paragraph_vector.pdf)\" (ICML2014)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Dai et al. \"[Document Embedding with Paragraph Vectors](https://fb56552f-a-62cb3a1a-s-sites.googlegroups.com/site/deeplearningworkshopnips2014/68.pdf)\" (NIPS2014 WS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Word as Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Distribution Hypothesis 分布仮説 [Firth 195?]\n",
    "- NLM [Bengio+ 2003]\n",
    "- Collobert & Weston 2008\n",
    "- Mnih & Hinton\n",
    "- Distributional Semantics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[Mikolov+ 2013] \"[Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "simple models, large data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- [Meyer 2016] \"[How exactly does word2vec work?](http://www.1-4-5.net/~dmm/ml/how_does_word2vec_work.pdf)\"\n",
    "- [\\[1402.3722\\] word2vec Explained: deriving Mikolov et al.'s negative-sampling word-embedding method](https://arxiv.org/abs/1402.3722)\n",
    "- [\\[1411.2738\\] word2vec Parameter Learning Explained](https://arxiv.org/abs/1411.2738)\n",
    "- [Vector Representations of Words  |  TensorFlow](https://www.tensorflow.org/tutorials/word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "You should listen to the man himself: [RRP #1: Tomáš Mikolov on word2vec and AI research at Microsoft, Google, Facebook | RARE Technologies](https://rare-technologies.com/rrp-1-tomas-mikolov-on-word2vec-and-ai-research-at-microsoft-google-facebook/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](https://cdn-images-1.medium.com/max/638/1*Bjtqi5sgc-pE8bB80IAkeA.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](http://livedoor.blogimg.jp/tak_tak0/imgs/a/3/a386ba59.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CBOW : Given context words, predict a word\n",
    "- Skip-gram : Given a word, predict its context words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Paragraph Vector [Le&Mikolov 2014]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The term doc2vec was popularised by Gensim\n",
    "[*](https://arxiv.org/abs/1607.05368)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Distributed Memory (PV-DM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://cdn-ak.f.st-hatena.com/images/fotolife/t/test1025/20140531/20140531225027.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Unique vector per each paragraph\n",
    "\n",
    "---\n",
    "\n",
    "- CBOW : Given context words, predict a word\n",
    "- PV-DM : Given context words **PLUS PARAGRAPH**, predict a word\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "- Two steps\n",
    "    - 1. Training:  Word & paragraph vectors, weights\n",
    "    - 2. Inference: New vector for new paragpraph input -> lock word vectors and weights, optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Distributed Bag of Words (PV-DBOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://camo.qiitausercontent.com/7fb968809d07aa24616b158c4dbd785c9008ebe9/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139343731352f37663138666265312d303537302d633031322d303930332d6364313633356434393530662e706e67)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Skip-gram : Given a word, predict its context words\n",
    "- PV-DBOW: Given **a paragraph**, predict its context words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Evaluation\n",
    "\n",
    "- Original paper\n",
    "    - Sentiment Anayisis : One sentence & IMDb review\n",
    "    - Information Retrieval : *Decide if a doc should be retrieved given a query*\n",
    "- Good results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### More Evaluation\n",
    "\n",
    "Dai et al. \"[Document Embedding with Paragraph Vectors](https://fb56552f-a-62cb3a1a-s-sites.googlegroups.com/site/deeplearningworkshopnips2014/68.pdf)\" (NIPS2014 WS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Compare with LDA\n",
    "- Document Similarity Tasks\n",
    "    - 1. Wikipedia\n",
    "    - 2. arXiv\n",
    "- Good results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Like word2vec, Vector Operation on Paragraph Vectors ...!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "** \"Lady Gaga\" - \"American\" + \"Japanese\" = \"Ayumi Hamasaki\" ...! **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "![](./6_doc2vec.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Our Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](https://www.kdnuggets.com/images/cartoon-machine-learning-what-they-think.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[gensim/doc2vec-IMDB.ipynb at develop · RaRe-Technologies/gensim](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-IMDB.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Pre-trained Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[jhlau/doc2vec: Python scripts for training/testing paragraph vectors](https://github.com/jhlau/doc2vec)\n",
    "\n",
    "* English Wikipedia DBOW (1.4GB)\n",
    "* Associated Press News DBOW (0.6GB)\n",
    "\n",
    "Jey Han Lau and Timothy Baldwin (2016). An Empirical Evaluation of doc2vec with Practical Insights into Document Embedding Generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ... Need to use forked version of gensim to load pre-trained model\n",
    "- - [Using pre-trained word2vec models in doc2vec · Issue #1338 · RaRe-Technologies/gensim](https://github.com/RaRe-Technologies/gensim/issues/1338)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Implementing your own"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- [Googling doc2vec - Luminis Amsterdam](https://amsterdam.luminis.eu/2017/01/16/googling-doc2vec/)\n",
    "- [Implementing doc2vec - Luminis Amsterdam](https://amsterdam.luminis.eu/2017/01/30/implementing-doc2vec/)\n",
    "- [Coding doc2vec - Luminis Amsterdam](https://amsterdam.luminis.eu/2017/02/21/coding-doc2vec/)\n",
    "    - [tensorflow/5_word2vec.ipynb at master · tensorflow/tensorflow](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/5_word2vec.ipynb)\n",
    "    - [blog-doc2vec/pvdbow.ipynb at master · luminis-ams/blog-doc2vec](https://github.com/luminis-ams/blog-doc2vec/blob/master/pvdbow.ipynb)\n",
    "- [A look at the source code of gensim doc2vec - Luminis Amsterdam](https://amsterdam.luminis.eu/2017/12/01/look-source-code-gensim-doc2vec/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Word Mover's Distance\n",
    "\n",
    "- [Kusner+ 2015] \"[From Word Embeddings To Document Distances](http://proceedings.mlr.press/v37/kusnerb15.pdf)\" (ICML2015)\n",
    "    - [implementation](https://github.com/mkusner/wmd)\n",
    "    - [presentation video](http://videolectures.net/icml2015_kusner_document_distances/)\n",
    "- [Huang+ 2016] \"[Supervised Word Mover's Distance](http://papers.nips.cc/paper/6138-supervised-word-movers-distance)\" (NIPS2016)\n",
    "\n",
    "![](https://github.com/mkusner/wmd/raw/master/fig1.png)\n",
    "![](https://cdn-ak.f.st-hatena.com/images/fotolife/y/yubessy/20170109/20170109172058.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](https://fasttext.cc/img/fasttext-logo-color-web.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[fastText](https://fasttext.cc/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://upload.wikimedia.org/wikipedia/commons/4/4d/Smoothed_LDA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- [Alvarez 2017] \"[A review of word embedding and document similarity algorithms applied to academic text](http://ad-publications.informatik.uni-freiburg.de/theses/Bachelor_Jon_Ezeiza_2017.pdf)\" (Bachelor's Thesis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- POS Information\n",
    "- word order? syntax?\n",
    "- [Representation learning for very short texts using weighted word embedding aggregation - ScienceDirect](https://www.sciencedirect.com/science/article/pii/S0167865516301362)\n",
    "    - *weighing word embeddings using idf*"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
